services:
  # The Transcription Engine
  whisper-live:
    image: ghcr.io/collabora/whisperlive-cpu:latest
    container_name: whisper-live
    ports:
      - "9090:9090"
    command: python3 run_server.py --port 9090 --backend faster_whisper
    # GPU configuration moved to docker-compose.gpu.yml
    # By default, this runs on CPU

  # Our Sales AI Web Application
  sales-ai-web:
    build: .
    container_name: sales-ai-web
    ports:
      - "8080:8080"
    environment:
      - WHISPER_HOST=whisper-live
      - WHISPER_PORT=9090
      - HOST=0.0.0.0
      - PORT=8080
      # Pass the API key from the host environment
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      # Local AI Configuration
      - LLM_PROVIDER=local
      - LOCAL_AI_BASE_URL=http://local-ai:8080/v1
      - LOCAL_AI_MODEL=llama3-8b
    depends_on:
      - whisper-live
    volumes:
      - ./src:/app/src  # Enable hot-reloading for development

  # Local Inference Engine
  local-ai:
    image: localai/localai:v2.24.0-ffmpeg-core
    container_name: local-ai
    ports:
      - "8081:8080"
    environment:
      - MODELS_PATH=/build/models
      - DEBUG=true
    volumes:
      - ./models:/build/models:cached
    command: ["/usr/bin/local-ai"]

