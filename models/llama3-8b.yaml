name: llama3-8b
backend: llama-cpp
parameters:
  model: llama-3-8b.Q4_K_M.gguf
  gpu_layers: -1
  stop:
    - "<|eot_id|>"
    - "<|end_of_text|>"
context_size: 4096
f16: true
mmap: true
download_files:
  - filename: llama-3-8b.Q4_K_M.gguf
    uri: https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
template:
  chat: |
    <|begin_of_text|><|start_header_id|>system<|end_header_id|>

    {{.System}}<|eot_id|><|start_header_id|>user<|end_header_id|>

    {{.Input}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
  completion: |
    {{.Input}}
